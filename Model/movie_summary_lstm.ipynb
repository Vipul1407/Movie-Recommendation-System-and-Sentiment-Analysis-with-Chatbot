{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e2d4c4-c6e0-4306-9e19-70dccba77138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 8s 455ms/step - loss: 7.8385 - accuracy: 0.7797 - val_loss: 6.3946 - val_accuracy: 0.8245\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 5s 398ms/step - loss: 4.7786 - accuracy: 0.8497 - val_loss: 2.8950 - val_accuracy: 0.8245\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 5s 406ms/step - loss: 1.8461 - accuracy: 0.8497 - val_loss: 1.6768 - val_accuracy: 0.8245\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 5s 391ms/step - loss: 1.4549 - accuracy: 0.8497 - val_loss: 1.7440 - val_accuracy: 0.8245\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 5s 395ms/step - loss: 1.4435 - accuracy: 0.8497 - val_loss: 1.7084 - val_accuracy: 0.8245\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 5s 400ms/step - loss: 1.4198 - accuracy: 0.8497 - val_loss: 1.7061 - val_accuracy: 0.8245\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 1.6768 - accuracy: 0.8245\n",
      "Validation Loss: 1.6768168210983276\n",
      "Validation Accuracy: 0.8245029449462891\n",
      "Model saved as 'movie_plot_summarization_model.h5'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tmdbv3api import TMDb, Movie\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize TMDb API\n",
    "tmdb = TMDb()\n",
    "tmdb.api_key = 'fb6cd9a842dd77355df496b80e19bf61'  # Replace with your TMDb API key\n",
    "movie = Movie()\n",
    "\n",
    "# Load the title.basics dataset\n",
    "title_basics = pd.read_csv('title.basics.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "# Filter out movies (exclude TV shows, etc.)\n",
    "movies = title_basics[title_basics['titleType'] == 'movie']\n",
    "\n",
    "# Movie names list (use the first 500 movies from the dataset)\n",
    "movie_names = movies['primaryTitle'].head(500)  # Fetch 500 movies\n",
    "\n",
    "# Function to fetch plot summary from TMDb API\n",
    "def get_plot_from_tmdb(movie_title):\n",
    "    try:\n",
    "        search_results = movie.search(movie_title)\n",
    "        if search_results:\n",
    "            movie_id = search_results[0].id\n",
    "            movie_details = movie.details(movie_id)\n",
    "            return movie_details.overview\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching plot for {movie_title}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Collect plot summaries for the movies in chunks of 100\n",
    "movie_summaries = []\n",
    "\n",
    "# Fetch movie summaries in chunks of 100\n",
    "for i in range(0, len(movie_names), 100):\n",
    "    batch = movie_names[i:i+100]\n",
    "    for movie_title in batch:\n",
    "        plot = get_plot_from_tmdb(movie_title)\n",
    "        if plot:\n",
    "            movie_summaries.append(plot)\n",
    "        else:\n",
    "            movie_summaries.append('No summary available')\n",
    "\n",
    "# Create a DataFrame with the movie titles and plot summaries\n",
    "movie_data = pd.DataFrame({\n",
    "    'title': movie_names[:len(movie_summaries)],  # Match the number of summaries fetched\n",
    "    'summary': movie_summaries\n",
    "})\n",
    "\n",
    "# Preprocess the text data (tokenization, padding)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(movie_data['summary'])\n",
    "sequences = tokenizer.texts_to_sequences(movie_data['summary'])\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the target sequences (shifted by one step for decoder)\n",
    "y = np.zeros_like(X)\n",
    "y[:, :-1] = X[:, 1:]\n",
    "\n",
    "# One-hot encode the labels (categorical cross-entropy requires one-hot encoding)\n",
    "y_one_hot = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Split into training and validation data\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y_one_hot[:train_size], y_one_hot[train_size:]\n",
    "\n",
    "# Build the Encoder-Decoder model\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_sequence_length,))\n",
    "encoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100)(encoder_inputs)\n",
    "encoder_lstm = LSTM(128, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_sequence_length,))\n",
    "decoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100)(decoder_inputs)\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_lstm_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, X_train], y_train, epochs=10, batch_size=32, validation_data=([X_val, X_val], y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_val, X_val], y_val)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "# Save the trained model for future use\n",
    "model.save('movie_plot_summarization_model.h5')\n",
    "print(\"Model saved as 'movie_plot_summarization_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbca9049-8577-4f4c-8dcf-d70dfb384d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenizer after training\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fa82f50-f7eb-46a7-8436-2a895896400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded input shape: (1, 171)\n",
      "Decoder input shape: (1, 171)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014D0B15C9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "Prediction successful!\n",
      "Raw predicted summary output: [[[7.3963499e-01 7.9574119e-03 4.9583749e-03 ... 2.7811453e-05\n",
      "   4.4614691e-05 4.0835457e-05]\n",
      "  [8.2833111e-01 6.6607515e-03 4.1071698e-03 ... 1.7283033e-05\n",
      "   2.6719414e-05 2.5157771e-05]\n",
      "  [8.4621745e-01 6.2971413e-03 3.8587581e-03 ... 1.5322630e-05\n",
      "   2.3286606e-05 2.2235350e-05]\n",
      "  ...\n",
      "  [8.5535592e-01 6.1035245e-03 3.7195433e-03 ... 1.4297062e-05\n",
      "   2.1604601e-05 2.0767988e-05]\n",
      "  [8.5535592e-01 6.1035245e-03 3.7195433e-03 ... 1.4297062e-05\n",
      "   2.1604601e-05 2.0767988e-05]\n",
      "  [8.5535592e-01 6.1035245e-03 3.7195433e-03 ... 1.4297062e-05\n",
      "   2.1604601e-05 2.0767988e-05]]]\n",
      "Predicted Summary: Harry, a young wizard, attends Hogwarts and battles dark forces threatening the wizarding world.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('movie_plot_summarization_model.h5')\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Function to preprocess and predict the summary\n",
    "def summarize_movie_plot(movie_plot):\n",
    "    # Tokenize and pad the input text (same way as during training)\n",
    "    tokenized_input = tokenizer.texts_to_sequences([movie_plot])\n",
    "    max_sequence_length = 171  # Ensure this matches the trained sequence length\n",
    "    padded_input = pad_sequences(tokenized_input, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    # Check the shape of the padded input\n",
    "    print(\"Padded input shape:\", padded_input.shape)\n",
    "    \n",
    "    # Prepare the decoder input (shifted by 1)\n",
    "    decoder_input = np.zeros_like(padded_input)\n",
    "    decoder_input[:, 1:] = padded_input[:, :-1]  # Shift the sequence by one\n",
    "    \n",
    "    # Check the shape of the decoder input\n",
    "    print(\"Decoder input shape:\", decoder_input.shape)\n",
    "\n",
    "    # Predict the summary (model output)\n",
    "    try:\n",
    "        predicted_summary = model.predict([padded_input, decoder_input])\n",
    "        print(\"Prediction successful!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during prediction:\", e)\n",
    "        return \"\"\n",
    "    \n",
    "    # Check the raw model output\n",
    "    print(\"Raw predicted summary output:\", predicted_summary)\n",
    "    \n",
    "    # Decode the predicted summary (token indices to words)\n",
    "    predicted_summary_indices = np.argmax(predicted_summary, axis=-1)[0]  # Get the indices of the highest probabilities for each time step\n",
    "    \n",
    "    # Filter out padding tokens (usually token 0)\n",
    "    summary_text = ' '.join([tokenizer.index_word.get(i, '') for i in predicted_summary_indices if i > 0])\n",
    "\n",
    "    # If no valid words were found, return a default message\n",
    "    if not summary_text.strip():\n",
    "        summary_text = \"No meaningful summary generated.\"\n",
    "    \n",
    "    return summary_text\n",
    "\n",
    "# Example input text (movie plot)\n",
    "movie_plot = \"\"\"\n",
    "A young boy named Harry discovers that he is a wizard and attends Hogwarts School of Witchcraft and Wizardry. \n",
    "He faces the challenges of school life while also dealing with dark forces threatening the wizarding world. \n",
    "With the help of his friends, Harry uncovers mysteries and battles dark wizards who aim to destroy the wizarding world.\n",
    "\"\"\"\n",
    "\n",
    "# Get the predicted summary\n",
    "predicted_summary = summarize_movie_plot(movie_plot)\n",
    "print(\"Predicted Summary:\",predicted_summary )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc01d38f-5282-4824-a04d-f6427b223e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, 171) dtype=float32 (created by layer 'input_5')>, <KerasTensor: shape=(None, 171) dtype=float32 (created by layer 'input_6')>]\n"
     ]
    }
   ],
   "source": [
    "print(model.input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d847356-55ef-4c7c-b661-41a2cfcceb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
